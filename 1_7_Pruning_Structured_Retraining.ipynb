{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "m7SE4a2gI2nK",
      "metadata": {
        "id": "m7SE4a2gI2nK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7C8Tbxv_I3TB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C8Tbxv_I3TB",
        "outputId": "a4511fe1-9060-4d81-f4b1-c4df98c34229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixed random seed: 42\n"
          ]
        }
      ],
      "source": [
        "def fix_random_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"Fixed random seed: {seed}\")\n",
        "\n",
        "fix_random_seed(42)\n",
        "\n",
        "# For deterministic DataLoader behavior\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "rGsmDtHRI4GH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGsmDtHRI4GH",
        "outputId": "faa052e0-c46b-4b2c-f591-263345bce3ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 45000\n",
            "Validation set size: 5000\n",
            "Test set size: 7500\n",
            "Small test size: 2500\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ==========================================================\n",
        "# Configuration\n",
        "# ==========================================================\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "BATCH_SIZE = 128\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(42)\n",
        "\n",
        "# ==========================================================\n",
        "# Transforms\n",
        "# ==========================================================\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_mean, imagenet_std),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_mean, imagenet_std),\n",
        "])\n",
        "\n",
        "# ==========================================================\n",
        "# Datasets and Splits\n",
        "# ==========================================================\n",
        "# Load once (without transform)\n",
        "full_train = torchvision.datasets.CIFAR100(root='./data', train=True, download=True)\n",
        "full_test = torchvision.datasets.CIFAR100(root='./data', train=False, download=True)\n",
        "\n",
        "# Split indices using a fixed seed generator\n",
        "train_indices, val_indices = random_split(range(len(full_train)), [45000, 5000], generator=g)\n",
        "test_indices, small_test_indices = random_split(range(len(full_test)), [7500, 2500], generator=g)\n",
        "\n",
        "# Wrap each subset with its own dataset and transform\n",
        "train_dataset = Subset(\n",
        "    torchvision.datasets.CIFAR100(root='./data', train=True, transform=train_transform),\n",
        "    train_indices.indices\n",
        ")\n",
        "val_dataset = Subset(\n",
        "    torchvision.datasets.CIFAR100(root='./data', train=True, transform=test_transform),\n",
        "    val_indices.indices\n",
        ")\n",
        "test_dataset = Subset(\n",
        "    torchvision.datasets.CIFAR100(root='./data', train=False, transform=test_transform),\n",
        "    test_indices.indices\n",
        ")\n",
        "small_test_dataset = Subset(\n",
        "    torchvision.datasets.CIFAR100(root='./data', train=False, transform=test_transform),\n",
        "    small_test_indices.indices\n",
        ")\n",
        "\n",
        "print(\"Training set size:\", len(train_dataset))\n",
        "print(\"Validation set size:\", len(val_dataset))\n",
        "print(\"Test set size:\", len(test_dataset))\n",
        "print(\"Small test size:\", len(small_test_dataset))\n",
        "\n",
        "# ==========================================================\n",
        "# Dataloaders\n",
        "# ==========================================================\n",
        "def get_loader(dataset, shuffle):\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g\n",
        "    )\n",
        "\n",
        "train_loader = get_loader(train_dataset, shuffle=True)\n",
        "val_loader = get_loader(val_dataset, shuffle=False)\n",
        "test_loader = get_loader(test_dataset, shuffle=False)\n",
        "small_test_loader = get_loader(small_test_dataset, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "kTBBH4rVI4-2",
      "metadata": {
        "id": "kTBBH4rVI4-2"
      },
      "outputs": [],
      "source": [
        "def plot_weight_histograms(model, title):\n",
        "    # Collect (layer_name, weights) pairs\n",
        "    layer_weights = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n",
        "            layer_weights.append((name, module.weight.detach().cpu().numpy().flatten()))\n",
        "\n",
        "    # Plot histograms\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    for i, (name, w) in enumerate(layer_weights[:11]):  # limit to first 11 for readability\n",
        "        plt.subplot(5, 6, i + 1)\n",
        "        plt.hist(w, bins=40, color='blue', alpha=0.7)\n",
        "        plt.title(name, fontsize=9)  # <-- actual layer name\n",
        "        plt.tight_layout()\n",
        "    plt.suptitle(title, y=1.02)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "qmEA4XmFJpka",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmEA4XmFJpka",
        "outputId": "f619a689-67ea-4784-dffe-50133ace5b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features.0.weight: torch.Size([52, 3, 3, 3])\n",
            "features.3.weight: torch.Size([101, 52, 3, 3])\n",
            "features.6.weight: torch.Size([196, 101, 3, 3])\n",
            "features.8.weight: torch.Size([186, 196, 3, 3])\n",
            "features.11.weight: torch.Size([361, 186, 3, 3])\n",
            "features.13.weight: torch.Size([345, 361, 3, 3])\n",
            "features.16.weight: torch.Size([330, 345, 3, 3])\n",
            "features.18.weight: torch.Size([512, 330, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "state_dict = torch.load('models/vgg11_cifar100_pruned_structured.pt', map_location='cpu')\n",
        "for k, v in state_dict.items():\n",
        "    if 'weight' in k and len(v.shape) == 4:\n",
        "        print(f\"{k}: {v.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ba3KLk2kJ0Eo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba3KLk2kJ0Eo",
        "outputId": "f056b948-232d-4069-b45c-a9039dc280c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(52, 101, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(101, 196, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(196, 186, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (11): Conv2d(186, 361, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): Conv2d(361, 345, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (14): ReLU(inplace=True)\n",
              "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (16): Conv2d(345, 330, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(330, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=100, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def make_vgg11_pruned():\n",
        "    cfg = [52, 'M', 101, 'M', 196, 186, 'M', 361, 345, 'M', 330, 512, 'M']\n",
        "\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            layers += [nn.Conv2d(in_channels, v, kernel_size=3, padding=1),\n",
        "                       nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    features = nn.Sequential(*layers)\n",
        "\n",
        "    # Classifier for CIFAR-100\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Linear(512 * 1 * 1, 4096),  # adjust if you used adaptive pooling\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(4096, 100)\n",
        "    )\n",
        "\n",
        "    model = models.vgg.VGG(features, num_classes=100)\n",
        "    return model\n",
        "\n",
        "# Instantiate pruned model\n",
        "vgg11_pruned = make_vgg11_pruned()\n",
        "\n",
        "# Load the pruned weights\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
        "state_dict = torch.load('models/vgg11_cifar100_pruned_structured.pt', map_location=device)\n",
        "vgg11_pruned.load_state_dict(state_dict)\n",
        "vgg11_pruned.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "kpQFmakyI6_p",
      "metadata": {
        "id": "kpQFmakyI6_p"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            preds = model(images)\n",
        "            preds = torch.argmax(preds, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    accuracy = correct / total * 100\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "Mh6xNNk2I7yo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh6xNNk2I7yo",
        "outputId": "68ad2237-c2fc-4fa9-d19d-f75f6e1aa0fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Structured Pruning Accuracy Before Retraining: 54.93%\n",
            "Structured Pruning Accuracy Before Retraining (Small): 54.60%\n"
          ]
        }
      ],
      "source": [
        "acc = evaluate(vgg11_pruned, test_loader, device)\n",
        "print(f\"Structured Pruning Accuracy Before Retraining: {acc:.2f}%\")\n",
        "\n",
        "acc = evaluate(vgg11_pruned, small_test_loader, device)\n",
        "print(f\"Structured Pruning Accuracy Before Retraining (Small): {acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "rMeIcNBZI8ne",
      "metadata": {
        "id": "rMeIcNBZI8ne"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "YFm_j7UEI9fc",
      "metadata": {
        "id": "YFm_j7UEI9fc"
      },
      "outputs": [],
      "source": [
        "base_lr = 0.04\n",
        "weight_decay = 5e-4\n",
        "num_epochs = 5\n",
        "optim = torch.optim.SGD(\n",
        "    vgg11_pruned.parameters(),\n",
        "    lr=base_lr,\n",
        "    momentum=0.9,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "scheduler = CosineAnnealingLR(optim, T_max=num_epochs, eta_min=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "PvRKxZ9nI-Mz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvRKxZ9nI-Mz",
        "outputId": "ae44043c-74d0-40e3-a5ca-a3944a6711c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 | Loss: 1.3185 | Train Acc: 62.44% | Val Acc: 56.26%\n",
            "Epoch 2/5 | Loss: 1.1066 | Train Acc: 68.00% | Val Acc: 60.74%\n",
            "Epoch 3/5 | Loss: 0.7368 | Train Acc: 78.24% | Val Acc: 64.42%\n",
            "Epoch 4/5 | Loss: 0.3972 | Train Acc: 87.92% | Val Acc: 68.96%\n",
            "Epoch 5/5 | Loss: 0.1966 | Train Acc: 93.85% | Val Acc: 71.36%\n"
          ]
        }
      ],
      "source": [
        "best_val = 0\n",
        "best_retrained = copy.deepcopy(vgg11_pruned)\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(vgg11_pruned, train_loader, criterion, optim, device)\n",
        "    scheduler.step()\n",
        "    val_acc = evaluate(vgg11_pruned, val_loader, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
        "    if val_acc > best_val:\n",
        "      best_retrained = copy.deepcopy(vgg11_pruned)\n",
        "      best_val = val_acc\n",
        "      torch.save(vgg11_pruned.state_dict(), \"vgg11_cifar100_pruned_structured_finetuned.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "qtiNoZBjI_Cy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtiNoZBjI_Cy",
        "outputId": "1f656b98-2360-4fc1-e90b-a108c3d686de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Structured Pruning Accuracy After Retraining: 71.64%\n",
            "Structured Pruning Accuracy After Retraining (Small): 70.40%\n"
          ]
        }
      ],
      "source": [
        "acc = evaluate(best_retrained, test_loader, device)\n",
        "print(f\"Structured Pruning Accuracy After Retraining: {acc:.2f}%\")\n",
        "\n",
        "acc = evaluate(best_retrained, small_test_loader, device)\n",
        "print(f\"Structured Pruning Accuracy After Retraining (Small): {acc:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
