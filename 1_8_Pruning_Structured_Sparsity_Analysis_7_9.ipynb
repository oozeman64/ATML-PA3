{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a6ce2e89",
      "metadata": {
        "id": "a6ce2e89"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "828dafbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "828dafbf",
        "outputId": "8b8cb8ed-9b6a-47aa-dd18-6802062df319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed random seed: 42\n"
          ]
        }
      ],
      "source": [
        "def fix_random_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"Fixed random seed: {seed}\")\n",
        "\n",
        "fix_random_seed(42)\n",
        "\n",
        "# For deterministic DataLoader behavior\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "638f27b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "638f27b9",
        "outputId": "644c8541-f947-4b82-a6b6-7ac802f1c108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:03<00:00, 43.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 45000\n",
            "Validation set size: 5000\n",
            "Test set size: 7500\n",
            "Small test size: 2500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ==========================================================\n",
        "# Configuration\n",
        "# ==========================================================\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "BATCH_SIZE = 128\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(42)\n",
        "\n",
        "# ==========================================================\n",
        "# Transforms\n",
        "# ==========================================================\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_mean, imagenet_std),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_mean, imagenet_std),\n",
        "])\n",
        "\n",
        "# ==========================================================\n",
        "# Datasets and Splits\n",
        "# ==========================================================\n",
        "# Load once (without transform)\n",
        "full_train = torchvision.datasets.CIFAR100(root='./data', train=True, download=True)\n",
        "full_test = torchvision.datasets.CIFAR100(root='./data', train=False, download=True)\n",
        "\n",
        "# Split indices using a fixed seed generator\n",
        "train_indices, val_indices = random_split(range(len(full_train)), [45000, 5000], generator=g)\n",
        "test_indices, small_test_indices = random_split(range(len(full_test)), [7500, 2500], generator=g)\n",
        "\n",
        "# Wrap each subset with its own dataset and transform\n",
        "train_dataset = Subset(\n",
        "    torchvision.datasets.CIFAR100(root='./data', train=True, transform=train_transform),\n",
        "    train_indices.indices\n",
        ")\n",
        "val_dataset = Subset(\n",
        "    torchvision.datasets.CIFAR100(root='./data', train=True, transform=test_transform),\n",
        "    val_indices.indices\n",
        ")\n",
        "test_dataset = Subset(\n",
        "    torchvision.datasets.CIFAR100(root='./data', train=False, transform=test_transform),\n",
        "    test_indices.indices\n",
        ")\n",
        "small_test_dataset = Subset(\n",
        "    torchvision.datasets.CIFAR100(root='./data', train=False, transform=test_transform),\n",
        "    small_test_indices.indices\n",
        ")\n",
        "\n",
        "print(\"Training set size:\", len(train_dataset))\n",
        "print(\"Validation set size:\", len(val_dataset))\n",
        "print(\"Test set size:\", len(test_dataset))\n",
        "print(\"Small test size:\", len(small_test_dataset))\n",
        "\n",
        "# ==========================================================\n",
        "# Dataloaders\n",
        "# ==========================================================\n",
        "def get_loader(dataset, shuffle):\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g\n",
        "    )\n",
        "\n",
        "train_loader = get_loader(train_dataset, shuffle=True)\n",
        "val_loader = get_loader(val_dataset, shuffle=False)\n",
        "test_loader = get_loader(test_dataset, shuffle=False)\n",
        "small_test_loader = get_loader(small_test_dataset, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7fd9e898",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fd9e898",
        "outputId": "b9a46988-d12e-47f5-e33f-cc32f84def11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg11-8a719046.pth\" to /root/.cache/torch/hub/checkpoints/vgg11-8a719046.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 507M/507M [00:05<00:00, 99.2MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (14): ReLU(inplace=True)\n",
              "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=100, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "vgg11 = models.vgg11(weights=models.VGG11_Weights.IMAGENET1K_V1)\n",
        "vgg11.classifier[6] = torch.nn.Linear(4096, 100)\n",
        "vgg11.classifier[5] = torch.nn.Dropout(p=0.5) # Dropout\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
        "vgg11.load_state_dict(torch.load('models/vgg11_cifar100_baseline.pt', weights_only=True, map_location=device))\n",
        "vgg11.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2afe2873",
      "metadata": {
        "id": "2afe2873"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            preds = model(images)\n",
        "            preds = torch.argmax(preds, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    accuracy = correct / total * 100\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6f420583",
      "metadata": {
        "id": "6f420583"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bd641d27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd641d27",
        "outputId": "d847ac6d-a6d0-4e87-f725-80a236a9bcac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Acc: 74.33\n",
            "Base Acc (small): 73.28\n"
          ]
        }
      ],
      "source": [
        "base_acc = evaluate(vgg11, test_loader, device)\n",
        "print(f\"Base Acc: {base_acc:.2f}\")\n",
        "base_acc = evaluate(vgg11, small_test_loader, device)\n",
        "print(f\"Base Acc (small): {base_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e58c3c8b",
      "metadata": {
        "id": "e58c3c8b"
      },
      "outputs": [],
      "source": [
        "def prune_layer_structured_L2(model, target_layer, amount=0.3):\n",
        "    \"\"\"\n",
        "    Performs structured channel-wise pruning on a Conv2d layer.\n",
        "    Prunes entire output channels with smallest L2-norms, and adjusts the next Conv2d layer.\n",
        "    \"\"\"\n",
        "    # Convert layer name (like \"features.3\") to module reference\n",
        "    modules = dict(model.named_modules())\n",
        "    if target_layer not in modules or not isinstance(modules[target_layer], torch.nn.Conv2d):\n",
        "        print(f\"Skipping {target_layer} (not Conv2d)\")\n",
        "        return model\n",
        "\n",
        "    conv = modules[target_layer]\n",
        "    with torch.no_grad():\n",
        "        # Compute L2 norm of each output channel (filter)\n",
        "        weight = conv.weight.data\n",
        "        out_channels = weight.shape[0]\n",
        "        num_prune = int(amount * out_channels)\n",
        "        if num_prune == 0:\n",
        "            return model\n",
        "\n",
        "        norms = torch.norm(weight.view(out_channels, -1), p=2, dim=1)\n",
        "        prune_idx = torch.argsort(norms)[:num_prune]\n",
        "        keep_idx = torch.argsort(norms)[num_prune:]\n",
        "\n",
        "        # Prune current layer (reduce output channels)\n",
        "        conv.weight = torch.nn.Parameter(weight[keep_idx].clone())\n",
        "        if conv.bias is not None:\n",
        "            conv.bias = torch.nn.Parameter(conv.bias.data[keep_idx].clone())\n",
        "        conv.out_channels = len(keep_idx)\n",
        "\n",
        "        # Find the next Conv2d and prune corresponding input channels\n",
        "        found = False\n",
        "        prev_name = target_layer\n",
        "        for name, module in modules.items():\n",
        "            if found and isinstance(module, torch.nn.Conv2d):\n",
        "                next_conv = module\n",
        "                next_weight = next_conv.weight.data[:, keep_idx, :, :].clone()\n",
        "                next_conv.weight = torch.nn.Parameter(next_weight)\n",
        "                next_conv.in_channels = len(keep_idx)\n",
        "                break\n",
        "            if name == target_layer:\n",
        "                found = True\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7f88fe02",
      "metadata": {
        "id": "7f88fe02"
      },
      "outputs": [],
      "source": [
        "conv_layers = [name for name, module in vgg11.named_modules() if isinstance(module, torch.nn.Conv2d)]\n",
        "\n",
        "with open('models/sensitivity_dict_structured.pkl', 'rb') as f:\n",
        "    sensitivity_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "77bad95c",
      "metadata": {
        "id": "77bad95c"
      },
      "outputs": [],
      "source": [
        "def get_prune_ratio(model, desired_channel_prune_ratio):\n",
        "    prune_amts = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
        "    conv_layers = [name for name, module in model.named_modules() if isinstance(module, torch.nn.Conv2d)]\n",
        "\n",
        "    desired_channel_prune = desired_channel_prune_ratio   # 30% of total conv output channels\n",
        "    min_layer_prune = 0.05        # don't prune less than 5%\n",
        "    max_layer_prune = 0.9         # cap per layer pruning\n",
        "\n",
        "\n",
        "    # 1) Gather Conv2d layers and their output channels\n",
        "    conv_out_channels = {}\n",
        "    conv_order = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            conv_out_channels[name] = module.out_channels\n",
        "            conv_order.append(name)\n",
        "\n",
        "    conv_layers = conv_order[:-1]\n",
        "    total_channels = sum(conv_out_channels[name] for name in conv_layers)\n",
        "    total_channels_to_prune = int(desired_channel_prune * total_channels)\n",
        "\n",
        "    R = {}\n",
        "    for name in conv_layers:\n",
        "        accs = []\n",
        "        for p in prune_amts:\n",
        "            acc = sensitivity_dict.get((name, p))\n",
        "            if acc is not None:\n",
        "                accs.append(acc)\n",
        "        if not accs or base_acc == 0:\n",
        "            R[name] = 1.0\n",
        "        else:\n",
        "            R[name] = (sum(accs) / len(accs)) / base_acc\n",
        "\n",
        "    # 3) Convert to sensitivity (S = 1 / R)\n",
        "    S = {n: 1.0 / max(R[n], 1e-6) for n in R}\n",
        "\n",
        "    # 4) Apply depth weighting (optional but realistic)\n",
        "    depth_weights = {name: 1.0 + (i / len(conv_layers)) for i, name in enumerate(conv_layers)}\n",
        "\n",
        "    # 5) Compute importance score (higher → more pruning)\n",
        "    score = {n: depth_weights[n] * S[n] for n in conv_layers}\n",
        "\n",
        "    # 6) Allocate total channels to prune across layers proportionally to score × out_channels\n",
        "    denom = sum(score[n] * conv_out_channels[n] for n in conv_layers)\n",
        "    remove_alloc = {}\n",
        "    for name in conv_layers:\n",
        "        remove_alloc[name] = total_channels_to_prune * (score[name] * conv_out_channels[name]) / denom\n",
        "\n",
        "    # 7) Compute per-layer pruning ratio (fraction of channels)\n",
        "    prune_ratio = {}\n",
        "    for name in conv_layers:\n",
        "        ratio = remove_alloc[name] / conv_out_channels[name]\n",
        "        ratio = min(max(ratio, min_layer_prune), max_layer_prune)\n",
        "        prune_ratio[name] = ratio\n",
        "\n",
        "    print(\"Final Channel Pruning Ratios:\")\n",
        "    for k, v in prune_ratio.items():\n",
        "        print(f\"{k}: {v*100:.2f}%\")\n",
        "\n",
        "    return prune_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e34501be",
      "metadata": {
        "id": "e34501be"
      },
      "outputs": [],
      "source": [
        "def train_5_epochs(model):\n",
        "    base_lr = 0.04\n",
        "    weight_decay = 5e-4\n",
        "    num_epochs = 5\n",
        "    optim = torch.optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=base_lr,\n",
        "        momentum=0.9,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    scheduler = CosineAnnealingLR(optim, T_max=num_epochs, eta_min=1e-5)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val = 0\n",
        "    best_retrained = copy.deepcopy(model)\n",
        "    for epoch in range(num_epochs):\n",
        "        _, _ = train_epoch(model, train_loader, criterion, optim, device)\n",
        "        scheduler.step()\n",
        "        val_acc = evaluate(model, val_loader, device)\n",
        "        if val_acc > best_val:\n",
        "            best_retrained = copy.deepcopy(model)\n",
        "            best_val = val_acc\n",
        "\n",
        "    return best_retrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b3e9c4a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3e9c4a1",
        "outputId": "aa89ceaa-1225-4dcf-faf1-4cbef6b251e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sparsity: 70.0% Channels Removed\n",
            "Final Channel Pruning Ratios:\n",
            "features.0: 45.73%\n",
            "features.3: 50.45%\n",
            "features.6: 55.31%\n",
            "features.8: 64.03%\n",
            "features.11: 69.09%\n",
            "features.13: 76.14%\n",
            "features.16: 83.02%\n",
            "Structured Pruning Test Acc with 70.0% Channels Removed Without Retraining: 5.00%\n",
            "Structured Pruning Test Acc with 70.0% Channels Removed With Retraining: 67.53%\n",
            "\n",
            "Sparsity: 80.0% Channels Removed\n",
            "Final Channel Pruning Ratios:\n",
            "features.0: 52.26%\n",
            "features.3: 57.66%\n",
            "features.6: 63.21%\n",
            "features.8: 73.18%\n",
            "features.11: 78.96%\n",
            "features.13: 87.02%\n",
            "features.16: 90.00%\n",
            "Structured Pruning Test Acc with 80.0% Channels Removed Without Retraining: 1.32%\n",
            "Structured Pruning Test Acc with 80.0% Channels Removed With Retraining: 64.15%\n",
            "\n",
            "Sparsity: 90.0% Channels Removed\n",
            "Final Channel Pruning Ratios:\n",
            "features.0: 58.80%\n",
            "features.3: 64.87%\n",
            "features.6: 71.11%\n",
            "features.8: 82.32%\n",
            "features.11: 88.83%\n",
            "features.13: 90.00%\n",
            "features.16: 90.00%\n",
            "Structured Pruning Test Acc with 90.0% Channels Removed Without Retraining: 0.99%\n",
            "Structured Pruning Test Acc with 90.0% Channels Removed With Retraining: 58.53%\n"
          ]
        }
      ],
      "source": [
        "sparsities = [0.7, 0.8, 0.9]\n",
        "pruning_dict = {}\n",
        "pruning_finetuning_dict = {}\n",
        "for sparsity in sparsities:\n",
        "    print(f\"\\nSparsity: {sparsity*100}% Channels Removed\")\n",
        "    vgg_pruned = copy.deepcopy(vgg11)\n",
        "\n",
        "    conv_layers = [name for name, m in vgg_pruned.named_modules() if isinstance(m, torch.nn.Conv2d)]\n",
        "    conv_layers = conv_layers[:-1]  # skip last conv\n",
        "\n",
        "    prune_ratio = get_prune_ratio(vgg_pruned, sparsity)\n",
        "\n",
        "    for name in conv_layers:\n",
        "        vgg_pruned = prune_layer_structured_L2(vgg_pruned, name, amount=prune_ratio[name])\n",
        "\n",
        "    acc = evaluate(vgg_pruned, test_loader, device)\n",
        "    pruning_dict[sparsity] = acc\n",
        "    print(f\"Structured Pruning Test Acc with {sparsity*100}% Channels Removed Without Retraining: {acc:.2f}%\")\n",
        "\n",
        "    vgg_pruned_retrained = train_5_epochs(vgg_pruned)\n",
        "    acc = evaluate(vgg_pruned_retrained, test_loader, device)\n",
        "    pruning_finetuning_dict[sparsity] = acc\n",
        "    print(f\"Structured Pruning Test Acc with {sparsity*100}% Channels Removed With Retraining: {acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Pruning Dict\")\n",
        "print(pruning_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjkSnWhfXrrO",
        "outputId": "29b8b438-aa5a-4247-bf5a-d6883eef612b"
      },
      "id": "JjkSnWhfXrrO",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruning Dict\n",
            "{0.7: 5.0, 0.8: 1.32, 0.9: 0.9866666666666666}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Pruning FineTuning Dict\")\n",
        "print(pruning_finetuning_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5dQQOUiXvhv",
        "outputId": "82100264-2408-434e-b4b1-e0c8b731f02c"
      },
      "id": "E5dQQOUiXvhv",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruning FineTuning Dict\n",
            "{0.7: 67.53333333333333, 0.8: 64.14666666666666, 0.9: 58.53333333333334}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('structured_pruning_no_tune_sparsity_dict_7_9.pkl', 'wb') as f:\n",
        "    pickle.dump(pruning_dict, f)\n",
        "\n",
        "with open('structured_pruning_fine_tuned_sparsity_dict_7_9.pkl', 'wb') as f:\n",
        "    pickle.dump(pruning_finetuning_dict, f)"
      ],
      "metadata": {
        "id": "-03-5Pqvb9FV"
      },
      "id": "-03-5Pqvb9FV",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('structured_pruning_no_tune_sparsity_dict_7_9.pkl')\n",
        "files.download('structured_pruning_fine_tuned_sparsity_dict_7_9.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "vX_klrvZcNOP",
        "outputId": "2cdfc5e6-74a5-415c-a7ed-b7b2efca8e34"
      },
      "id": "vX_klrvZcNOP",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a6d6ccd6-85de-4df7-96fa-99be3ec48e5d\", \"structured_pruning_no_tune_sparsity_dict_7_9.pkl\", 70)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_820f0b18-24fc-476d-89af-afd2323e5efa\", \"structured_pruning_fine_tuned_sparsity_dict_7_9.pkl\", 70)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}